data = training_set)
R_lineal
#Info
summary(R_lineal)
# Predicting the Test set results
y_pred = predict(regressor, newdata = test_set)
# Visualising the Training set results
library(ggplot2)
ggplot() +
geom_point(aes(x = training_set$YearsExperience, y = training_set$Salary),
colour = 'red') +
geom_line(aes(x = training_set$YearsExperience, y = predict(R_lineal, newdata = training_set)),
colour = 'blue') +
ggtitle('Salary vs Experience (Training set)') +
xlab('Years of experience') +
ylab('Salary')
# Visualising the Test set results
library(ggplot2)
ggplot() +
geom_point(aes(x = test_set$YearsExperience, y = test_set$Salary),
colour = 'red') +
geom_line(aes(x = training_set$YearsExperience, y = predict(R_lineal, newdata = training_set)),
colour = 'blue') +
ggtitle('Salary vs Experience (Test set)') +
xlab('Years of experience') +
ylab('Salary')
#Residuals
par(mfrow = c(1,1))
residuals_rlin <- rstandard(R_lineal)
adjusted_values_rlin <- fitted(R_lineal)
plot(adjusted_values_rlin, residuals_rlin)
#Revisar normalidad de los errores
qqnorm(residuals_rlin)
qqline(residuals_rlin)
#Normality test Kolmogorov-Smirnov
ks.test(x = residuals_rlin,"pnorm", mean(residuals_rlin), sd(residuals_rlin))
#Normality test Kolmogorov-Smirnov modification test Lilliefors
library("nortest")
lillie.test(x = residuals_rlin)
#Jarque Bera Test
library("tseries")
jarque.bera.test(x = residuals_rlin)
# Simple Linear Regression
setwd("C:/Users/BIO/Desktop/Data_Analysis_Comparation/Clase_Regresion/\1_Regresion_Simple")
# Importing the dataset
dataset = read.csv('Salary_Data.csv')
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Salary, SplitRatio = 2/3)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
# training_set = scale(training_set)
# test_set = scale(test_set)
# Fitting Simple Linear Regression to the Training set
R_lineal = lm(formula = Salary ~ YearsExperience,
data = training_set)
R_lineal
#Info
summary(R_lineal)
# Predicting the Test set results
y_pred = predict(regressor, newdata = test_set)
# Visualising the Training set results
library(ggplot2)
ggplot() +
geom_point(aes(x = training_set$YearsExperience, y = training_set$Salary),
colour = 'red') +
geom_line(aes(x = training_set$YearsExperience, y = predict(R_lineal, newdata = training_set)),
colour = 'blue') +
ggtitle('Salary vs Experience (Training set)') +
xlab('Years of experience') +
ylab('Salary')
# Visualising the Test set results
library(ggplot2)
ggplot() +
geom_point(aes(x = test_set$YearsExperience, y = test_set$Salary),
colour = 'red') +
geom_line(aes(x = training_set$YearsExperience, y = predict(R_lineal, newdata = training_set)),
colour = 'blue') +
ggtitle('Salary vs Experience (Test set)') +
xlab('Years of experience') +
ylab('Salary')
#Residuals
par(mfrow = c(1,1))
residuals_rlin <- rstandard(R_lineal)
adjusted_values_rlin <- fitted(R_lineal)
plot(adjusted_values_rlin, residuals_rlin)
#Revisar normalidad de los errores
qqnorm(residuals_rlin)
qqline(residuals_rlin)
#Normality test Kolmogorov-Smirnov
ks.test(x = residuals_rlin,"pnorm", mean(residuals_rlin), sd(residuals_rlin))
#Normality test Kolmogorov-Smirnov modification test Lilliefors
library("nortest")
lillie.test(x = residuals_rlin)
#Jarque Bera Test
library("tseries")
jarque.bera.test(x = residuals_rlin)
# Simple Linear Regression
setwd("C:/Users/BIO/Desktop/Data_Analysis_Comparation/Clase_Regresion/1_Regresion_Simple")
# Importing the dataset
dataset = read.csv('Salary_Data.csv')
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Salary, SplitRatio = 2/3)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
# training_set = scale(training_set)
# test_set = scale(test_set)
# Fitting Simple Linear Regression to the Training set
R_lineal = lm(formula = Salary ~ YearsExperience,
data = training_set)
R_lineal
#Info
summary(R_lineal)
# Predicting the Test set results
y_pred = predict(regressor, newdata = test_set)
# Visualising the Training set results
library(ggplot2)
ggplot() +
geom_point(aes(x = training_set$YearsExperience, y = training_set$Salary),
colour = 'red') +
geom_line(aes(x = training_set$YearsExperience, y = predict(R_lineal, newdata = training_set)),
colour = 'blue') +
ggtitle('Salary vs Experience (Training set)') +
xlab('Years of experience') +
ylab('Salary')
# Visualising the Test set results
library(ggplot2)
ggplot() +
geom_point(aes(x = test_set$YearsExperience, y = test_set$Salary),
colour = 'red') +
geom_line(aes(x = training_set$YearsExperience, y = predict(R_lineal, newdata = training_set)),
colour = 'blue') +
ggtitle('Salary vs Experience (Test set)') +
xlab('Years of experience') +
ylab('Salary')
#Residuals
par(mfrow = c(1,1))
residuals_rlin <- rstandard(R_lineal)
adjusted_values_rlin <- fitted(R_lineal)
plot(adjusted_values_rlin, residuals_rlin)
#Revisar normalidad de los errores
qqnorm(residuals_rlin)
qqline(residuals_rlin)
#Normality test Kolmogorov-Smirnov
ks.test(x = residuals_rlin,"pnorm", mean(residuals_rlin), sd(residuals_rlin))
#Normality test Kolmogorov-Smirnov modification test Lilliefors
library("nortest")
lillie.test(x = residuals_rlin)
#Jarque Bera Test
library("tseries")
jarque.bera.test(x = residuals_rlin)
library(ggplot2)
ggplot() +
geom_point(aes(x = training_set$YearsExperience, y = training_set$Salary),
colour = 'red') +
geom_line(aes(x = training_set$YearsExperience, y = predict(R_lineal, newdata = training_set)),
colour = 'blue') +
ggtitle('Salary vs Experience (Training set)') +
xlab('Years of experience') +
ylab('Salary')
library(ggplot2)
ggplot() +
geom_point(aes(x = test_set$YearsExperience, y = test_set$Salary),
colour = 'red') +
geom_line(aes(x = training_set$YearsExperience, y = predict(R_lineal, newdata = training_set)),
colour = 'blue') +
ggtitle('Salary vs Experience (Test set)') +
xlab('Years of experience') +
ylab('Salary')
#Residuals
par(mfrow = c(1,1))
residuals_rlin <- rstandard(R_lineal)
adjusted_values_rlin <- fitted(R_lineal)
plot(adjusted_values_rlin, residuals_rlin)
# Multiple Linear Regression
setwd("C:/Users/BIO/Desktop/Data_Analysis_Comparation/Clase_Regresion/2_Regresion_Multiple")
# Importing the dataset
dataset = read.csv('Data_Mul_Reg.csv')
# Multiple Linear Regression
setwd("C:/Users/BIO/Desktop/Data_Analysis_Comparation/Clase_Regresion/2_Regresion_Multiple")
# Importing the dataset
dataset = read.csv('Data_Mul_Reg.csv')
View(dataset)
View(dataset)
View(dataset)
# Multiple Linear Regression
setwd("C:/Users/BIO/Desktop/Data_Analysis_Comparation/Clase_Regresion/2_Regresion_Multiple")
# Importing the dataset
dataset = read.csv('Data_Mul_Reg.csv')
#Eliminar una columna
library(dplyr)
dataset <- select(dataset, -X)
View(dataset)
names(dataset)
#Rename variables
colnames(dataset)[colnames(dataset)=="Q.Spend"] <- "Quality.Spend"
View(dataset)
setwd("C:/Users/BIO/Google Drive (leorod3321@gmail.com)/2019_LeoWork/Machine Learning A-Z Template Folder/Part 2 - Regression/Section 5 - Multiple Linear Regression/Multiple_Linear_Regression")
# Multiple Linear Regression
# Importing the dataset
dataset = read.csv('50_Startups.csv')
View(dataset)
View(dataset)
#Set wd
setwd("C:/Users/BIO/Desktop/Data_Analysis_Comparation/Clase_Datos")
# Import csv
dataset = read.csv('Data_Missing.csv')
View(dataset)
View(dataset)
View(dataset)
dataset$Age = ifelse(is.na(dataset$Age),
ave(dataset$Age, FUN = function(x) mean(x, na.rm = TRUE)),
dataset$Age)
dataset$Salary = ifelse(is.na(dataset$Salary),
ave(dataset$Salary, FUN = function(x) mean(x, na.rm = TRUE)),
dataset$Salary)
View(dataset)
View(dataset)
# Multiple Linear Regression
setwd("C:/Users/BIO/Desktop/Data_Analysis_Comparation/Clase_Regresion/2_Regresion_Multiple")
# Importing the dataset
dataset = read.csv('Data_Mul_Reg.csv')
#Eliminar una columna
library(dplyr)
dataset <- select(dataset, -X)
#Rename variables
colnames(dataset)[colnames(dataset)=="Q.Spend"] <- "Quality.Spend"
# Encoding categorical data
dataset$State = factor(dataset$State,
levels = c('New York', 'California', 'Florida'),
labels = c(1, 2, 3))
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Profit, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
# training_set = scale(training_set)
# test_set = scale(test_set)
names(training_set)
# Predicting the Test set results
y_pred = predict(regressor, newdata = test_set)
summary(regressor)
# Multiple Linear Regression
setwd("C:/Users/BIO/Desktop/Data_Analysis_Comparation/Clase_Regresion/2_Regresion_Multiple")
# Importing the dataset
dataset = read.csv('Data_Mul_Reg.csv')
#Eliminar una columna
library(dplyr)
dataset <- select(dataset, -X)
#Rename variables
colnames(dataset)[colnames(dataset)=="Q.Spend"] <- "Quality.Spend"
# Encoding categorical data
dataset$State = factor(dataset$State,
levels = c('New York', 'California', 'Florida'),
labels = c(1, 2, 3))
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Profit, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
# training_set = scale(training_set)
# test_set = scale(test_set)
# Fitting Multiple Linear Regression to the Training set
regressor = lm(formula = Profit ~ .,
data = training_set)
# Predicting the Test set results
y_pred = predict(regressor, newdata = test_set)
summary(regressor)
plot(regressor)
s
plot(regressor)
plot(regressor)
mfrow = c(2,2)
plot(regressor)
layout(matrix(c(2),2,2))
plot(regressor)
par(mfrow = c(1,1))
plot(regressor)
layout(matrix(c(1,2,3,4),2,2)) # optional 4 graphs/page
plot(regressor)
layout(matrix(c(1,2,3,4),2,2)) # optional 4 graphs/page
layout(matrix(c(1),1,1)) # optional 1 graphs/page
# Plot the actual points
layout(matrix(c(1,2,3,4),2,2)) # optional 4 graphs/page
plot(regressor)
plot(predict(regressor),training_set$Profit,
col = "blue", pch=1,
xlab="Predicción",ylab="actual", main="Est. Punt. Lec.Crí. Mét.: Ridge")
abline(a=0,b=1, col=153, lty=5)
layout(matrix(c(1),1,1)) # optional 1 graphs/page
plot(predict(regressor),training_set$Profit,
col = "blue", pch=1,
xlab="Predicción",ylab="actual", main="Est. Punt. Lec.Crí. Mét.: Ridge")
abline(a=0,b=1, col=153, lty=5)
plot(predict(regressor),training_set$Profit,
col = "blue", pch=1,
xlab="Predicción",ylab="actual", main="Est. Punt. Lec.Crí. Mét.: Ridge")
plot(predict(regressor),test_set$Profit,
col = "blue", pch=1,
xlab="Predicción",ylab="actual", main="Est. Punt. Lec.Crí. Mét.: Ridge")
plot(predict(regressor),test_set$Profit,
col = "blue", pch=1,
)
plot(predict(regressor),training_set$Profit,
col = "blue", pch=1,
xlab="Predicción",ylab="actual", main="Est. Punt. Lec.Crí. Mét.: Ridge")
abline(a=0,b=1, col=153, lty=5)
plot(predict(regressor),training_set$Profit,
col = "blue", pch=1,
xlab="Prediction",ylab="Real", main="Prediction comparison vs actual behavior")
abline(a=0,b=1, col=153, lty=5)
plot(predict(regressor),training_set$Profit,
col = "blue", pch=1,
xlab="Prediction",ylab="Real", main="Prediction vs actual behavior")
abline(a=0,b=1, col=153, lty=5)
#Predict residuals
residuos<-rstandard(regressor)
hist(residuos) # histograma de los residuos estandarizados
boxplot(residuos) # diagrama de cajas de los residuos estandarizados
hist(residuos)
boxplot(residuos)
help(boxplot)
boxplot(residuos, boxwex = 0.5, col = c("orange", "yellow"))
boxplot(residuos, boxwex = 0.5, col = c("orange", "yellow"),
main = "Guinea Pigs' Tooth Growth")
boxplot(residuos, boxwex = 0.5, col = c("orange", "yellow"),
main = "Box",
xlab = "Vitamin C dose mg", ylab = "tooth length")
boxplot(residuos, boxwex = 0.5, col = c("orange", "yellow"),
main = "Box",
xlab = "Error", ylab = "length")
#Predict residuals
residuos<-rstandard(regressor)
hist(residuos)
boxplot(residuos, boxwex = 0.5, col = c("orange", "yellow"),
main = "Box",
xlab = "Error", ylab = "length",
sep = ":", lex.order = TRUE, ylim = c(0, 35), yaxs = "i")
boxplot(residuos, boxwex = 0.5, col = c("orange", "yellow"),
main = "Box",
xlab = "Error", ylab = "length",
sep = ":", lex.order = TRUE, ylim = c(-3, 3), yaxs = "i")
boxplot(residuos, boxwex = 0.5, col = c("orange", "yellow"),
main = "Box",
xlab = "Error", ylab = "length",
sep = "-", lex.order = TRUE, ylim = c(-3, 3), yaxs = "i")
boxplot(residuos, boxwex = 0.5, col = c("red", "yellow"),
main = "Box",
xlab = "Error", ylab = "length",
sep = ":", lex.order = TRUE, ylim = c(-3, 3), yaxs = "i")
boxplot(residuos, boxwex = 0.5, col = c("red", "blue"),
main = "Box",
xlab = "Error", ylab = "length",
sep = ":", lex.order = TRUE, ylim = c(-3, 3), yaxs = "i")
boxplot(residuos, boxwex = 0.5, col = c("blue", "red"),
main = "Box",
xlab = "Error", ylab = "length",
sep = ":", lex.order = TRUE, ylim = c(-3, 3), yaxs = "i")
# Multiple Linear Regression
setwd("C:/Users/BIO/Desktop/Data_Analysis_Comparation/Clase_Regresion/2_Regresion_Multiple")
# Importing the dataset
dataset = read.csv('Data_Mul_Reg.csv')
#Eliminar una columna
library(dplyr)
dataset <- select(dataset, -X)
#Rename variables
colnames(dataset)[colnames(dataset)=="Q.Spend"] <- "Quality.Spend"
# Encoding categorical data
dataset$State = factor(dataset$State,
levels = c('New York', 'California', 'Florida'),
labels = c(1, 2, 3))
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Profit, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
# training_set = scale(training_set)
# test_set = scale(test_set)
# Fitting Multiple Linear Regression to the Training set
regressor = lm(formula = Profit ~ .,
data = training_set)
# Predicting the Test set results
y_pred = predict(regressor, newdata = test_set)
summary(regressor)
# Plot the actual points
layout(matrix(c(1,2,3,4),2,2)) # optional 4 graphs/page
plot(regressor)
# Plot the actual points
layout(matrix(c(1),1,1)) # optional 1 graphs/page
#Prediction vs actual behavior
plot(predict(regressor),training_set$Profit,
col = "blue", pch=1,
xlab="Prediction",ylab="Real", main="Prediction vs actual behavior")
abline(a=0,b=1, col=153, lty=5)
#Predict residuals
residuos<-rstandard(regressor)
hist(residuos)
boxplot(residuos, boxwex = 0.5, col = c("blue", "red"),
main = "Box",
xlab = "Error", ylab = "length",
sep = ":", lex.order = TRUE, ylim = c(-3, 3), yaxs = "i")
#Predict residuals
residuals<-rstandard(regressor)
hist(residuals)
boxplot(residuals, boxwex = 0.5, col = c("blue", "red"),
main = "Box",
xlab = "Error", ylab = "length",
sep = ":", lex.order = TRUE, ylim = c(-3, 3), yaxs = "i")
#Residuals
par(mfrow = c(1,1))
adjusted_values_rmul <- fitted(regressor)
plot(adjusted_values_rmul, residuals)
#Normality test Kolmogorov-Smirnov
ks.test(x = residuals,"pnorm", mean(residuals), sd(residuals))
#Normality test Kolmogorov-Smirnov modification test Lilliefors
library("nortest")
lillie.test(x = residuals)
#Jarque Bera Test
library("tseries")
jarque.bera.test(x = residuals)
#Residuals
par(mfrow = c(1,1))
adjusted_values_rmul <- fitted(regressor)
plot(adjusted_values_rmul, residuals)
# Multiple Linear Regression
setwd("C:/Users/BIO/Desktop/Data_Analysis_Comparation/Clase_Regresion/2_Regresion_Multiple")
# Importing the dataset
dataset = read.csv('Data_Mul_Reg.csv')
#Eliminar una columna
library(dplyr)
dataset <- select(dataset, -X)
#Rename variables
colnames(dataset)[colnames(dataset)=="Q.Spend"] <- "Quality.Spend"
# Encoding categorical data
dataset$State = factor(dataset$State,
levels = c('New York', 'California', 'Florida'),
labels = c(1, 2, 3))
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Profit, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
# training_set = scale(training_set)
# test_set = scale(test_set)
# Fitting Multiple Linear Regression to the Training set
regressor = lm(formula = Profit ~ .,
data = training_set)
# Predicting the Test set results
y_pred = predict(regressor, newdata = test_set)
summary(regressor)
# Plot the actual points
layout(matrix(c(1,2,3,4),2,2)) # optional 4 graphs/page
plot(regressor)
# Plot the actual points
layout(matrix(c(1),1,1)) # optional 1 graphs/page
#Prediction vs actual behavior
plot(predict(regressor),training_set$Profit,
col = "blue", pch=1,
xlab="Prediction",ylab="Real", main="Prediction vs actual behavior")
abline(a=0,b=1, col=153, lty=5)
#Predict residuals
residuals<-rstandard(regressor)
hist(residuals)
boxplot(residuals, boxwex = 0.5, col = c("blue", "red"),
main = "Box",
xlab = "Error", ylab = "length",
sep = ":", lex.order = TRUE, ylim = c(-3, 3), yaxs = "i")
#Residuals
par(mfrow = c(1,1))
adjusted_values_rmul <- fitted(regressor)
plot(adjusted_values_rmul, residuals)
#Normality test Kolmogorov-Smirnov
ks.test(x = residuals,"pnorm", mean(residuals), sd(residuals))
#Normality test Kolmogorov-Smirnov modification test Lilliefors
library("nortest")
lillie.test(x = residuals)
#Jarque Bera Test
library("tseries")
jarque.bera.test(x = residuals)
boxplot(residuals, boxwex = 0.5, col = c("blue", "red"),
main = "Box",
xlab = "Error", ylab = "length",
sep = ":", lex.order = TRUE, ylim = c(-3, 3), yaxs = "i")
#Residuals
# Multiple Linear Regression
setwd("C:/Users/BIO/Desktop/Data_Analysis_Comparation/Clase_Regresion/2_Regresion_Multiple")
# Importing the dataset
dataset = read.csv('Data_Mul_Reg.csv')
View(dataset)
View(dataset)
names(dataset)
sd(Q.Spend)
sd(dataset$Q.Spend)
